{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Croissant Dataset Explorer, DenseNet Embedder, and RO-Crate Packager\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load and explore a Croissant dataset metadata.\n",
    "2. Access the actual image data.\n",
    "3. Use a simple DenseNet from torchvision to generate embeddings.\n",
    "4. Package the results and the notebook into a research object (RO-Crate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import pathlib\n",
    "import shutil\n",
    "from fairscape_cli.models.rocrate import GenerateROCrate, AppendCrate\n",
    "from fairscape_cli.models.software import GenerateSoftware\n",
    "from fairscape_cli.models.dataset import GenerateDataset\n",
    "from fairscape_cli.models.computation import GenerateComputation\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Croissant Dataset\n",
    "\n",
    "First, let's load the Croissant metadata file and explore what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: CM4AI_Multi_Treatment_IF_Dataset\n",
      "Dataset URL: https://cm4ai.org\n"
     ]
    }
   ],
   "source": [
    "croissant_file = \"cm4ai_if_images_croissant.json\"\n",
    "\n",
    "dataset = mlc.Dataset(jsonld=croissant_file)\n",
    "\n",
    "print(f\"Dataset name: {dataset.metadata.name}\")\n",
    "print(f\"Dataset URL: {dataset.metadata.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Dataset Structure\n",
    "\n",
    "Let's examine the resources (data files) and record sets (logical data groupings) in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 RESOURCES (Data Files):\n",
      "--------------------------------------------------\n",
      "FileObject(uuid=\"manifest-csv\")\n",
      "  UUID: manifest-csv\n",
      "  Description: Manifest file mapping archive paths to experimental metadata\n",
      "FileObject(uuid=\"untreated-archive\")\n",
      "  UUID: untreated-archive\n",
      "  Description: Zip archive containing untreated control images organized by channel\n",
      "FileObject(uuid=\"paclitaxel-archive\")\n",
      "  UUID: paclitaxel-archive\n",
      "  Description: Zip archive containing paclitaxel treatment images organized by channel\n",
      "FileObject(uuid=\"vorinostat-archive\")\n",
      "  UUID: vorinostat-archive\n",
      "  Description: Zip archive containing vorinostat treatment images organized by channel\n",
      "FileSet(uuid=\"all-image-files\")\n",
      "  UUID: all-image-files\n",
      "  Description: All immunofluorescence images across all treatments and channels\n",
      "\n",
      "\n",
      "📊 RECORD SETS (Data Structures):\n",
      "--------------------------------------------------\n",
      "\n",
      "Record Set: all_images\n",
      "  Description: Complete dataset with all treatments, channels, and experimental metadata. The user should filter this record set in their code to create train/test splits based on the 'split' field.\n",
      "  Sample Fields:\n",
      "    - plate: https://schema.org/Integer\n",
      "      Description: Plate identifier number for the experiment\n",
      "    - well: https://schema.org/Text\n",
      "      Description: Well position on the plate (e.g., G11, E10, D4)\n",
      "    - treatment: https://schema.org/Text\n",
      "      Description: Treatment condition: untreated, paclitaxel, or vorinostat\n",
      "    - hpa_antibody_id: https://schema.org/Text\n",
      "      Description: Human Protein Atlas antibody identifier\n",
      "    - ensembl_id: https://schema.org/Text\n",
      "      Description: Ensembl gene identifier\n"
     ]
    }
   ],
   "source": [
    "print(\"📁 RESOURCES (Data Files):\")\n",
    "print(\"-\" * 50)\n",
    "for i, resource in enumerate(dataset.metadata.distribution):\n",
    "    print(resource)\n",
    "    print(f\"  UUID: {resource.uuid}\")\n",
    "    print(f\"  Description: {getattr(resource, 'description', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\\n📊 RECORD SETS (Data Structures):\")\n",
    "print(\"-\" * 50)\n",
    "for record_set in dataset.metadata.record_sets:\n",
    "    print(f\"\\nRecord Set: {record_set.name}\")\n",
    "    if hasattr(record_set, 'description') and record_set.description:\n",
    "        print(f\"  Description: {record_set.description}\")\n",
    "    \n",
    "    print(f\"  Sample Fields:\")\n",
    "    for field in record_set.fields[:5]:\n",
    "        print(f\"    - {field.name}: {field.data_types[0] if field.data_types else 'unknown type'}\")\n",
    "        if hasattr(field, 'description') and field.description:\n",
    "            print(f\"      Description: {field.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Actual Data\n",
    "\n",
    "This will take a long time. It downloads all the images and unzips them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total records loaded: 51440\n"
     ]
    }
   ],
   "source": [
    "all_records = list(dataset.records(record_set=\"all_images\"))\n",
    "print(f\"✅ Total records loaded: {len(all_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset for Embedding Generation\n",
    "\n",
    "We'll create a dataset class that loads images for embedding generation and extracts their ARK identifiers for provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbeddingDataset(Dataset):\n",
    "    def __init__(self, records, record_set_prefix, transform=None, max_images=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.image_ids = []\n",
    "        self.metadata = []\n",
    "        \n",
    "        for i, record in enumerate(records):\n",
    "            if max_images and len(self.image_paths) >= max_images:\n",
    "                break\n",
    "                \n",
    "            image_path = record.get(f\"{record_set_prefix}/full_path\")\n",
    "            if image_path and os.path.exists(image_path):\n",
    "                self.image_paths.append(image_path)\n",
    "                \n",
    "                image_id = os.path.basename(image_path)\n",
    "                if isinstance(image_id, bytes):\n",
    "                    image_id = image_id.decode('utf-8')\n",
    "                self.image_ids.append(image_id)\n",
    "                \n",
    "                def safe_decode(value, default=\"\"):\n",
    "                    if isinstance(value, bytes):\n",
    "                        return value.decode('utf-8')\n",
    "                    elif value is None:\n",
    "                        return default\n",
    "                    else:\n",
    "                        return str(value)\n",
    "                \n",
    "                metadata = {\n",
    "                    'treatment': safe_decode(record.get(f\"{record_set_prefix}/treatment\")),\n",
    "                    'plate': safe_decode(record.get(f\"{record_set_prefix}/plate\")),\n",
    "                    'well': safe_decode(record.get(f\"{record_set_prefix}/well\")),\n",
    "                    'channel': safe_decode(record.get(f\"{record_set_prefix}/channel\")),\n",
    "                    'hpa_antibody_id': safe_decode(record.get(f\"{record_set_prefix}/hpa_antibody_id\")),\n",
    "                    'ensembl_id': safe_decode(record.get(f\"{record_set_prefix}/ensembl_id\")),\n",
    "                    'ark': safe_decode(record.get(f\"{record_set_prefix}/ark\"))\n",
    "                }\n",
    "                self.metadata.append(metadata)\n",
    "        \n",
    "        if len(self.metadata) > 0:\n",
    "            print(\"Sample metadata:\")\n",
    "            print(f\"  Treatment: {self.metadata[0]['treatment']}\")\n",
    "            print(f\"  Plate: {self.metadata[0]['plate']}\")\n",
    "            print(f\"  Well: {self.metadata[0]['well']}\")\n",
    "            print(f\"  Channel: {self.metadata[0]['channel']}\")\n",
    "            print(f\"  ARK ID: {self.metadata[0]['ark']}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        metadata = self.metadata[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            if self.transform:\n",
    "                image = self.transform(Image.new('RGB', (224, 224), (0, 0, 0)))\n",
    "            else:\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        return image, image_id, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up DenseNet Embedding Model\n",
    "\n",
    "We'll use a pre-trained DenseNet model and extract features from the layer before classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNetEmbedder(\n",
       "  (densenet): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DenseNetEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNetEmbedder, self).__init__()\n",
    "        self.densenet = models.densenet121(weights='IMAGENET1K_V1')\n",
    "        self.densenet.classifier = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.densenet(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DenseNetEmbedder().to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Image Embeddings\n",
    "\n",
    "Now let's generate embeddings for a small subset of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample metadata:\n",
      "  Treatment: Vorinostat\n",
      "  Plate: 1\n",
      "  Well: A10\n",
      "  Channel: \n",
      "  ARK ID: ark:59852/B2AI_1_Vorinostat_A10_R11_z01_blue\n",
      "Dataset size: 10\n",
      "Batch size: 16\n",
      "Number of batches: 1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "embedding_dataset = ImageEmbeddingDataset(\n",
    "    all_records, \n",
    "    \"all_images\", \n",
    "    transform=transform, \n",
    "    max_images=10 # Using a small number for this demo\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    embedding_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(embedding_dataset)}\")\n",
    "print(f\"Batch size: {dataloader.batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1/1 [00:04<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 10 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_list = []\n",
    "image_ids_list = []\n",
    "metadata_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, image_ids, metadata_batch) in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        embeddings = model(images)\n",
    "        \n",
    "        embeddings_list.extend(embeddings.cpu().numpy())\n",
    "        image_ids_list.extend(image_ids)\n",
    "        \n",
    "        batch_size = len(list(metadata_batch.values())[0])  \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            individual_metadata = {}\n",
    "            for key, value_list in metadata_batch.items():\n",
    "                individual_metadata[key] = value_list[i]\n",
    "            metadata_list.append(individual_metadata)\n",
    "\n",
    "print(f\"✅ Generated {len(embeddings_list)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Embeddings to Files\n",
    "\n",
    "Let's save the embeddings and metadata to TSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings saved to: ./densenet_embeddings/image_embeddings.tsv\n",
      "✅ Metadata saved to: ./densenet_embeddings/image_metadata.tsv\n",
      "\n",
      "Embeddings shape: (10, 1024)\n",
      "Metadata shape: (10, 8)\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./densenet_embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "embedding_dim = embeddings_array.shape[1]\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings_array, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
    "embeddings_df.insert(0, 'image_id', image_ids_list)\n",
    "\n",
    "embeddings_file = os.path.join(output_dir, \"image_embeddings.tsv\")\n",
    "embeddings_df.to_csv(embeddings_file, sep='\\t', index=False)\n",
    "\n",
    "metadata_rows = []\n",
    "for i, metadata in enumerate(metadata_list):\n",
    "    row = {'image_id': image_ids_list[i]}\n",
    "    row.update(metadata)\n",
    "    metadata_rows.append(row)\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata_rows)\n",
    "\n",
    "print(f\"✅ Embeddings saved to: {embeddings_file}\")\n",
    "print(f\"\\nEmbeddings shape: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create and Register an RO-Crate\n",
    "\n",
    "Now we will package the notebook and its outputs into an RO-Crate for portability and provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RO-Crate initialized at: densenet_embeddings\n",
      "✅ Registered Jupyter Notebook for Embedding Generation with GUID: ark:59852/embedding-notebook\n",
      "✅ Registered Image Embedding Generation Computation with GUID: ark:59852/embedding-computation\n",
      "✅ Registered Image Embedding Vectors (TSV) with GUID: ark:59852/embedding-vectors\n"
     ]
    }
   ],
   "source": [
    "rocrate_path = pathlib.Path('densenet_embeddings')\n",
    "notebook_name = \"demo.ipynb\"\n",
    "author_name = \"Notebook User\"\n",
    "\n",
    "crate_params = {\n",
    "    \"guid\": \"ark:59852/demo-if-embedding-rocrate\",\n",
    "    \"name\": \"DenseNet Image Embeddings from IF Data\",\n",
    "    \"organizationName\": \"CM4AI\",\n",
    "    \"projectName\": \"IF Image Embedding\",\n",
    "    \"description\": \"1024-dimensional image embeddings generated by a DenseNet model from a subset of the CM4AI IF image dataset.\",\n",
    "    \"keywords\": [\"embedding\", \"immunofluorescence\", \"densenet\", \"deep learning\"],\n",
    "    \"author\": author_name,\n",
    "    \"version\":\"1.0.0\", \n",
    "    \"path\": rocrate_path\n",
    "}\n",
    "\n",
    "crate = GenerateROCrate(**crate_params)\n",
    "print(f\"✅ RO-Crate initialized at: {rocrate_path}\")\n",
    "\n",
    "shutil.copy(notebook_name, rocrate_path / notebook_name)\n",
    "\n",
    "software_params = {\n",
    "    \"guid\": \"ark:59852/embedding-notebook\",\n",
    "    \"name\": \"Jupyter Notebook for Embedding Generation\",\n",
    "    \"author\": author_name,\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"description\": \"A Jupyter Notebook that loads IF images from a Croissant dataset, generates embeddings using DenseNet, and packages the results.\",\n",
    "    \"keywords\": [\"python\", \"jupyter\", \"pytorch\", \"mlcroissant\"],\n",
    "    \"fileFormat\": \"application/x-ipynb+json\",\n",
    "    'dateModified': pd.Timestamp.now().isoformat(),\n",
    "    \"filepath\": './densenet_embeddings/' + notebook_name,\n",
    "    \"cratePath\": rocrate_path\n",
    "}\n",
    "notebook_software = GenerateSoftware(**software_params)\n",
    "\n",
    "\n",
    "source_image_arks = metadata_df['ark'].unique().tolist()\n",
    "\n",
    "computation_params = {\n",
    "    \"guid\": \"ark:59852/embedding-computation\",\n",
    "    \"name\": \"Image Embedding Generation Computation\",\n",
    "    \"runBy\": author_name,\n",
    "    \"dateCreated\": pd.Timestamp.now().isoformat(),\n",
    "    \"description\": \"Execution of the embedding notebook to generate DenseNet vectors from IF images.\",\n",
    "    \"keywords\": [\"embedding\", \"densenet\", \"execution\"],\n",
    "    \"usedSoftware\": [notebook_software.guid],\n",
    "    \"usedDataset\": source_image_arks\n",
    "}\n",
    "embedding_computation = GenerateComputation(**computation_params)\n",
    "\n",
    "dataset_params = {\n",
    "    \"guid\": \"ark:59852/embedding-vectors\",\n",
    "    \"name\": \"Image Embedding Vectors (TSV)\",\n",
    "    \"description\": f\"A TSV file containing {embedding_dim}-dimensional embeddings for {len(embeddings_df)} images.\",\n",
    "    \"author\": author_name,\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"keywords\": [\"embedding\", \"vector\", \"tsv\"],\n",
    "    \"format\": \"text/tab-separated-values\",\n",
    "    \"datePublished\": pd.Timestamp.now().isoformat(),\n",
    "    \"filepath\": './densenet_embeddings/' + os.path.basename(embeddings_file),\n",
    "    \"derivedFrom\": source_image_arks,\n",
    "    \"generatedBy\": [embedding_computation.guid],\n",
    "    \"cratePath\": rocrate_path\n",
    "}\n",
    "embedding_dataset_component = GenerateDataset(**dataset_params)\n",
    "\n",
    "\n",
    "AppendCrate(\n",
    "    cratePath=rocrate_path,\n",
    "    elements=[\n",
    "        notebook_software,\n",
    "        embedding_computation,\n",
    "        embedding_dataset_component\n",
    "    ]\n",
    ")\n",
    "print(f\"✅ Registered {notebook_software.name} with GUID: {notebook_software.guid}\")\n",
    "print(f\"✅ Registered {embedding_computation.name} with GUID: {embedding_computation.guid}\")\n",
    "print(f\"✅ Registered {embedding_dataset_component.name} with GUID: {embedding_dataset_component.guid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully:\n",
    "1. **Loaded the Croissant dataset** and explored its structure.\n",
    "2. **Created a PyTorch dataset** for loading images.\n",
    "3. **Set up a DenseNet embedding model** using torchvision's pre-trained weights.\n",
    "4. **Generated embeddings** for a subset of images in the dataset.\n",
    "5. **Saved the results** to TSV files for further analysis.\n",
    "6. **Packaged the entire workflow** into an RO-Crate for portability and provenance.\n",
    "\n",
    "The output now includes:\n",
    "\n",
    "- `./densenet_embeddings/`: An RO-Crate directory containing:\n",
    "  - `ro-crate-metadata.json`: The metadata file describing the contents of the crate.\n",
    "  - `demo.ipynb`: A copy of this notebook, registered as the `Software` that performed the work.\n",
    "  - `image_embeddings.tsv`: The generated embeddings, registered as a `Dataset` derived from the source images and generated by the notebook.\n",
    "\n",
    "These embeddings and their corresponding RO-Crate can now be used for downstream tasks like clustering, classification, or similarity analysis, with a clear record of their origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
